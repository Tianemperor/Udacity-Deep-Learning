{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow\n",
    "   环境搭建：   \n",
    "        conda create -n tensorflow python=3.5    \n",
    "            source activate tensorflow    \n",
    "            conda install pandas matplotlib jupyter notebook scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello_constant = tf.constant('Hello World')\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(123, dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def run():\n",
    "\toutput = None\n",
    "\tx = tf.placeholder(tf.int32)\n",
    "\n",
    "\twith tf.Session() as sess:\n",
    "\t\toutput = sess.run(x, feed_dict={x: 123})\n",
    "\n",
    "\treturn output\n",
    "out = run()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(10.)\n",
    "y = tf.constant(2.)\n",
    "z = tf.subtract(tf.divide(x, y), tf.constant(1.))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\toutput = sess.run(z)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "#Udacity Solution\n",
    "import tensorflow as tf\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.divide(x, y), tf.cast(tf.constant(1), tf.float64))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<tf.Variable 'Variable_6:0' shape=() dtype=int32_ref>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.Variable(5)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_features  = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(init))\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow里的线性函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(n_features, n_labels):\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "def linear(input, w, b):\n",
    "    return tf.add(tf.matmul(input, w), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Loss: 10.069314002990723\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "    \n",
    "    mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "    #读取一万张图片\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "        #如果标签（0，1，2）存在，填加到列表\n",
    "        #(0, 0, 0, 0, 1, 0, 0, 0, 0)[:n_labels].any() 返回False\n",
    "        #(0, 1, 0, 0, 0, 0, 0, 0, 0)[:n_labels].any() 返回True\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "            \n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "#fetures 28*28=784\n",
    "n_features = 784\n",
    "\n",
    "n_labels = 3\n",
    "\n",
    "#Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "#Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "#Linear Function\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "#Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "    \n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    learning_rate = 0.08\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    _, l = sess.run([optimizer, loss], feed_dict = {features: train_features, labels: train_labels})\n",
    "    \n",
    "print('Loss: {}'.format(l))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65900117,  0.24243298,  0.09856589], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(softmax, feed_dict = {logits: logit_data})\n",
    "    return output\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### One-Hot Encoding\n",
    "采用scikit-learn的LabelBinarizer实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "labels = np.array([1,5,3,2,1,4,2,1,3])\n",
    "\n",
    "#创建编码器\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "#编码器找到类别并分配one-hot向量\n",
    "lb.fit(labels)\n",
    "\n",
    "#把目标(labels)转换成one-hot encoded向量\n",
    "lb.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(cross_entropy, feed_dict = {softmax: softmax_data, one_hot: one_hot_data})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784) <class 'numpy.ndarray'> (10000, 784) (10000, 10)\n",
      "[[-0.77327007  2.33807683 -1.66552627 ...,  0.49962536  0.51685196\n",
      "  -1.38834178]\n",
      " [ 0.44412798 -1.23280287 -0.12093657 ..., -1.96390283 -0.58952963\n",
      "   1.45713615]\n",
      " [-2.02368808 -1.36306787 -0.43683842 ...,  0.75053465  0.33054754\n",
      "  -0.41696873]\n",
      " ..., \n",
      " [-0.91441298 -0.30104902  0.39006051 ..., -0.1081707  -0.85398602\n",
      "  -0.74767083]\n",
      " [ 1.76962876 -1.11978626 -0.2817426  ..., -0.11449924 -2.57422066\n",
      "  -1.62547016]\n",
      " [-0.30794644  2.73987889  1.20813298 ...,  0.79665655 -1.05025947\n",
      "  -1.5225538 ]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784 #输入维度\n",
    "n_class = 10  #10个分类\n",
    "\n",
    "#导入数据\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)\n",
    "\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_class]))\n",
    "biases = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "print(train_features.shape, type(train_features), test_features.shape, test_labels.shape)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(sess.run(weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Minibatch with TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    #return: Batches of (Features, Labels)\n",
    "    a = []\n",
    "    assert len(features) == len(labels)\n",
    "    \n",
    "    for idx in range(int(len(features)/batch_size) + 1):\n",
    "        a.append([features[idx * batch_size: (idx + 1) * batch_size], labels[idx * batch_size: (idx + 1) * batch_size]])\n",
    "    return a\n",
    "\n",
    "from pprint import pprint\n",
    "#pprint 提供了打印出任何python数据结构类和方法\n",
    "#4 samples of features\n",
    "example_features = [\n",
    "    ['F11', 'F12', 'F13', 'F14'],\n",
    "    ['F21', 'F22', 'F23', 'F24'],\n",
    "    ['F31', 'F32', 'F33', 'F34'],\n",
    "    ['F41', 'F42', 'F43', 'F44']]\n",
    "\n",
    "#4 samples of labels\n",
    "example_labels = [\n",
    "    ['L11', 'L12'],\n",
    "    ['L21', 'L22'],\n",
    "    ['L31', 'L32'],\n",
    "    ['L41', 'L42']]\n",
    "\n",
    "#pprint prints data structures like 2d arrays, so they are easier to read\n",
    "pprint(batches(3, example_features, example_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniBatch on MNIST features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Test Accuracy: 0.05849999934434891\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784\n",
    "n_class = 10\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)\n",
    "\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_class])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_class]))\n",
    "bias = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "#计算准确率\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#set batch size\n",
    "batch_size = 128\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for batch_features, batch_labels in  batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict = {features: batch_features, labels: batch_labels})\n",
    "    \n",
    "    test_accuracy = sess.run(accuracy, feed_dict = {features: test_features, labels: test_labels})\n",
    "    \n",
    "print(\"Test Accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0    - Cost: 13.1     Valid Accuracy: 0.132\n",
      "Epoch: 1    - Cost: 11.8     Valid Accuracy: 0.151\n",
      "Epoch: 2    - Cost: 10.8     Valid Accuracy: 0.162\n",
      "Epoch: 3    - Cost: 10.0     Valid Accuracy: 0.169\n",
      "Epoch: 4    - Cost: 9.32     Valid Accuracy: 0.178\n",
      "Epoch: 5    - Cost: 8.74     Valid Accuracy: 0.186\n",
      "Epoch: 6    - Cost: 8.24     Valid Accuracy: 0.197\n",
      "Epoch: 7    - Cost: 7.82     Valid Accuracy: 0.214\n",
      "Epoch: 8    - Cost: 7.45     Valid Accuracy: 0.229\n",
      "Epoch: 9    - Cost: 7.12     Valid Accuracy: 0.244\n",
      "Test Accuracy: 0.2402999997138977\n"
     ]
    }
   ],
   "source": [
    "from tensorflow .examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    #print cost and validation accuracy of an epoch\n",
    "    \n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(epoch_i, current_cost, valid_accuracy))\n",
    "    #format()函数，格式限定符（语法是{}中带:号），填充与对齐，<,>分别是左对齐，右对齐，后面带宽度\n",
    "    \n",
    "n_input = 784\n",
    "n_class = 10\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)\n",
    "\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "features = tf.placeholder(tf.float32, [None, 784])\n",
    "labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_class]))\n",
    "bias = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict = feed_dict)\n",
    "            \n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "        \n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "    \n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0    - Cost: 12.1     Valid Accuracy: 0.142\n",
      "Epoch: 1    - Cost: 10.9     Valid Accuracy: 0.161\n",
      "Epoch: 2    - Cost: 10.0     Valid Accuracy: 0.178\n",
      "Epoch: 3    - Cost: 9.37     Valid Accuracy: 0.198\n",
      "Epoch: 4    - Cost: 8.82     Valid Accuracy: 0.215\n",
      "Epoch: 5    - Cost: 8.33     Valid Accuracy: 0.233\n",
      "Epoch: 6    - Cost: 7.91     Valid Accuracy: 0.251\n",
      "Epoch: 7    - Cost: 7.54     Valid Accuracy: 0.266\n",
      "Epoch: 8    - Cost: 7.21     Valid Accuracy: 0.285\n",
      "Epoch: 9    - Cost: 6.92     Valid Accuracy: 0.298\n",
      "Epoch: 10   - Cost: 6.66     Valid Accuracy: 0.317\n",
      "Epoch: 11   - Cost: 6.42     Valid Accuracy: 0.327\n",
      "Epoch: 12   - Cost: 6.21     Valid Accuracy: 0.339\n",
      "Epoch: 13   - Cost: 6.0      Valid Accuracy: 0.352\n",
      "Epoch: 14   - Cost: 5.82     Valid Accuracy: 0.362\n",
      "Epoch: 15   - Cost: 5.64     Valid Accuracy: 0.373\n",
      "Epoch: 16   - Cost: 5.48     Valid Accuracy: 0.384\n",
      "Epoch: 17   - Cost: 5.33     Valid Accuracy: 0.395\n",
      "Epoch: 18   - Cost: 5.19     Valid Accuracy: 0.408\n",
      "Epoch: 19   - Cost: 5.06     Valid Accuracy: 0.42 \n",
      "Epoch: 20   - Cost: 4.93     Valid Accuracy: 0.434\n",
      "Epoch: 21   - Cost: 4.82     Valid Accuracy: 0.445\n",
      "Epoch: 22   - Cost: 4.71     Valid Accuracy: 0.457\n",
      "Epoch: 23   - Cost: 4.6      Valid Accuracy: 0.465\n",
      "Epoch: 24   - Cost: 4.5      Valid Accuracy: 0.475\n",
      "Epoch: 25   - Cost: 4.41     Valid Accuracy: 0.484\n",
      "Epoch: 26   - Cost: 4.32     Valid Accuracy: 0.492\n",
      "Epoch: 27   - Cost: 4.23     Valid Accuracy: 0.498\n",
      "Epoch: 28   - Cost: 4.15     Valid Accuracy: 0.507\n",
      "Epoch: 29   - Cost: 4.07     Valid Accuracy: 0.513\n",
      "Epoch: 30   - Cost: 4.0      Valid Accuracy: 0.52 \n",
      "Epoch: 31   - Cost: 3.93     Valid Accuracy: 0.529\n",
      "Epoch: 32   - Cost: 3.86     Valid Accuracy: 0.532\n",
      "Epoch: 33   - Cost: 3.79     Valid Accuracy: 0.54 \n",
      "Epoch: 34   - Cost: 3.73     Valid Accuracy: 0.547\n",
      "Epoch: 35   - Cost: 3.67     Valid Accuracy: 0.552\n",
      "Epoch: 36   - Cost: 3.61     Valid Accuracy: 0.557\n",
      "Epoch: 37   - Cost: 3.55     Valid Accuracy: 0.562\n",
      "Epoch: 38   - Cost: 3.5      Valid Accuracy: 0.567\n",
      "Epoch: 39   - Cost: 3.44     Valid Accuracy: 0.572\n",
      "Epoch: 40   - Cost: 3.39     Valid Accuracy: 0.578\n",
      "Epoch: 41   - Cost: 3.34     Valid Accuracy: 0.582\n",
      "Epoch: 42   - Cost: 3.29     Valid Accuracy: 0.589\n",
      "Epoch: 43   - Cost: 3.25     Valid Accuracy: 0.594\n",
      "Epoch: 44   - Cost: 3.2      Valid Accuracy: 0.599\n",
      "Epoch: 45   - Cost: 3.16     Valid Accuracy: 0.604\n",
      "Epoch: 46   - Cost: 3.11     Valid Accuracy: 0.606\n",
      "Epoch: 47   - Cost: 3.07     Valid Accuracy: 0.61 \n",
      "Epoch: 48   - Cost: 3.03     Valid Accuracy: 0.613\n",
      "Epoch: 49   - Cost: 2.99     Valid Accuracy: 0.618\n",
      "Epoch: 50   - Cost: 2.96     Valid Accuracy: 0.623\n",
      "Epoch: 51   - Cost: 2.92     Valid Accuracy: 0.626\n",
      "Epoch: 52   - Cost: 2.88     Valid Accuracy: 0.63 \n",
      "Epoch: 53   - Cost: 2.85     Valid Accuracy: 0.634\n",
      "Epoch: 54   - Cost: 2.82     Valid Accuracy: 0.637\n",
      "Epoch: 55   - Cost: 2.78     Valid Accuracy: 0.64 \n",
      "Epoch: 56   - Cost: 2.75     Valid Accuracy: 0.642\n",
      "Epoch: 57   - Cost: 2.72     Valid Accuracy: 0.647\n",
      "Epoch: 58   - Cost: 2.69     Valid Accuracy: 0.651\n",
      "Epoch: 59   - Cost: 2.66     Valid Accuracy: 0.654\n",
      "Epoch: 60   - Cost: 2.63     Valid Accuracy: 0.657\n",
      "Epoch: 61   - Cost: 2.61     Valid Accuracy: 0.66 \n",
      "Epoch: 62   - Cost: 2.58     Valid Accuracy: 0.664\n",
      "Epoch: 63   - Cost: 2.55     Valid Accuracy: 0.666\n",
      "Epoch: 64   - Cost: 2.53     Valid Accuracy: 0.667\n",
      "Epoch: 65   - Cost: 2.5      Valid Accuracy: 0.668\n",
      "Epoch: 66   - Cost: 2.48     Valid Accuracy: 0.672\n",
      "Epoch: 67   - Cost: 2.45     Valid Accuracy: 0.674\n",
      "Epoch: 68   - Cost: 2.43     Valid Accuracy: 0.677\n",
      "Epoch: 69   - Cost: 2.41     Valid Accuracy: 0.68 \n",
      "Epoch: 70   - Cost: 2.39     Valid Accuracy: 0.681\n",
      "Epoch: 71   - Cost: 2.36     Valid Accuracy: 0.684\n",
      "Epoch: 72   - Cost: 2.34     Valid Accuracy: 0.687\n",
      "Epoch: 73   - Cost: 2.32     Valid Accuracy: 0.69 \n",
      "Epoch: 74   - Cost: 2.3      Valid Accuracy: 0.692\n",
      "Epoch: 75   - Cost: 2.28     Valid Accuracy: 0.694\n",
      "Epoch: 76   - Cost: 2.26     Valid Accuracy: 0.696\n",
      "Epoch: 77   - Cost: 2.25     Valid Accuracy: 0.697\n",
      "Epoch: 78   - Cost: 2.23     Valid Accuracy: 0.698\n",
      "Epoch: 79   - Cost: 2.21     Valid Accuracy: 0.701\n",
      "Epoch: 80   - Cost: 2.19     Valid Accuracy: 0.704\n",
      "Epoch: 81   - Cost: 2.17     Valid Accuracy: 0.706\n",
      "Epoch: 82   - Cost: 2.16     Valid Accuracy: 0.707\n",
      "Epoch: 83   - Cost: 2.14     Valid Accuracy: 0.71 \n",
      "Epoch: 84   - Cost: 2.12     Valid Accuracy: 0.711\n",
      "Epoch: 85   - Cost: 2.11     Valid Accuracy: 0.712\n",
      "Epoch: 86   - Cost: 2.09     Valid Accuracy: 0.714\n",
      "Epoch: 87   - Cost: 2.08     Valid Accuracy: 0.715\n",
      "Epoch: 88   - Cost: 2.06     Valid Accuracy: 0.715\n",
      "Epoch: 89   - Cost: 2.05     Valid Accuracy: 0.717\n",
      "Epoch: 90   - Cost: 2.03     Valid Accuracy: 0.718\n",
      "Epoch: 91   - Cost: 2.02     Valid Accuracy: 0.72 \n",
      "Epoch: 92   - Cost: 2.0      Valid Accuracy: 0.721\n",
      "Epoch: 93   - Cost: 1.99     Valid Accuracy: 0.722\n",
      "Epoch: 94   - Cost: 1.98     Valid Accuracy: 0.724\n",
      "Epoch: 95   - Cost: 1.96     Valid Accuracy: 0.725\n",
      "Epoch: 96   - Cost: 1.95     Valid Accuracy: 0.726\n",
      "Epoch: 97   - Cost: 1.94     Valid Accuracy: 0.727\n",
      "Epoch: 98   - Cost: 1.92     Valid Accuracy: 0.728\n",
      "Epoch: 99   - Cost: 1.91     Valid Accuracy: 0.73 \n",
      "Test Accuracy: 0.7312999963760376\n"
     ]
    }
   ],
   "source": [
    "from tensorflow .examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    #print cost and validation accuracy of an epoch\n",
    "    \n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(epoch_i, current_cost, valid_accuracy))\n",
    "    #format()函数，格式限定符（语法是{}中带:号），填充与对齐，<,>分别是左对齐，右对齐，后面带宽度\n",
    "    \n",
    "n_input = 784\n",
    "n_class = 10\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)\n",
    "\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "features = tf.placeholder(tf.float32, [None, 784])\n",
    "labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_class]))\n",
    "bias = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict = feed_dict)\n",
    "            \n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "        \n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "    \n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
